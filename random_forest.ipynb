{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedShuffleSplit, KFold\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix, ConfusionMatrixDisplay, f1_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from assemble_data import *\n",
    "from feature_extraction import *\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "path = os.getcwd()\n",
    "if path.endswith('Modeling'):\n",
    "    os.chdir('../APS_Data')\n",
    "    path = os.getcwd()\n",
    "\n",
    "accelerations = ['Ax', 'Ay', 'Az', 'Gx', 'Gy', 'Gz']    # Features list for creating string titles\n",
    "freq_funcs = ['MNF', 'MDF', 'PKF', 'MNP', 'TTP', 'FR', 'PSR', 'FMD', 'FMN']\n",
    "time_funcs = ['AAV', 'MAV', 'MMAV', 'SSI', 'VAR', 'ZC', 'SSC', 'WL', 'AAC', 'WAMP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_expanding_features(features, accel, misc):\n",
    "    cols = misc\n",
    "    cols.append('Trial')   \n",
    "    cols.append('MS Score')\n",
    "\n",
    "    for f in features: \n",
    "        for a in accel:\n",
    "            cols.append(a + '_' + f)   \n",
    "    table = pd.read_csv('Feature_Tables/features.csv', usecols=cols)  # concatenating aceeleration names and feature names\n",
    "    misc_table = pd.read_csv('Feature_Tables/features.csv', usecols=misc + ['Trial'])\n",
    "    \n",
    "    X = []\n",
    "    Y = []\n",
    "    trials = table['Trial'].unique()\n",
    "\n",
    "    for t in trials:\n",
    "        misc_data = misc_table[(misc_table['Trial'] == t)]  # categorical features\n",
    "        data = table[(table['Trial'] == t)]\n",
    "        y = []              \n",
    "        for v in data['MS Score'].values:               \n",
    "            # y.append(0 if v < 3 else 1 if v < 7 else 2)     # ternary class             \n",
    "            y.append(0 if v < 3 else 1)     # binary class              \n",
    "        Y.append(y)             \n",
    "        # creating label vector\n",
    "        data.drop(columns=['Trial', 'MS Score'], inplace=True)\n",
    "        misc_data.drop(columns=['Trial'], inplace=True)\n",
    "\n",
    "        cumulative_data = data.cumsum(axis=0)\n",
    "        cumulative_average_data = cumulative_data.divide(range(1, len(cumulative_data) + 1), axis=0)\n",
    "        subject_code = np.zeros(35)\n",
    "        subject_code[int(crop_name(t)[:2]) - 1] = 1\n",
    "        extended_code = np.tile(subject_code, (data.shape[0], 1))\n",
    "\n",
    "        X.append(np.concatenate((cumulative_average_data.values, data.values, extended_code), axis=1))\n",
    "        # X.append(np.concatenate((data.values, extended_code), axis=1)) # for current window\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "\n",
    "def extract_time_offset_features(features, accel, misc, n=1):\n",
    "    cols = []\n",
    "    cols.append('Trial')\n",
    "    cols.append('MS Score')\n",
    "\n",
    "    for f in features:\n",
    "        for a in accel:\n",
    "            cols.append(a + '_' + f)\n",
    "    # concatenating aceeleration names and feature names\n",
    "\n",
    "    data_table = pd.read_csv('Feature_Tables/features.csv', usecols=cols)\n",
    "    baseline_table = pd.read_csv('Feature_Tables/features_baseline.csv', usecols=cols)\n",
    "    misc_table = pd.read_csv('Feature_Tables/features.csv', usecols=misc + ['Trial'])\n",
    "    base_misc_table = pd.read_csv('Feature_Tables/features_baseline.csv', usecols=misc + ['Trial'])\n",
    "    # read data into pandas DF's\n",
    "    X = []\n",
    "    Y = []\n",
    "    trials = data_table['Trial'].unique()\n",
    "\n",
    "    for t in trials:\n",
    "        base = baseline_table[baseline_table['Trial'] == t]     # baseline data\n",
    "        base_misc_data = base_misc_table[(base_misc_table['Trial'] == t)]\n",
    "        data = data_table[(data_table['Trial'] == t)]   # IMU data\n",
    "        data = pd.concat([base, data], axis=0)\n",
    "        misc_data = misc_table[(misc_table['Trial'] == t)]  # categorical features\n",
    "        misc_data = pd.concat([base_misc_data, misc_data], axis=0)\n",
    "        # indexing data tables by trial\n",
    "        y = []\n",
    "        for v in data['MS Score'].values:\n",
    "            # y.append(0 if v < 2 else 1 if v < 5 else 2)     # ternary class\n",
    "            y.append(0 if v < 3 else 1)     # binary class\n",
    "        Y.append(y)\n",
    "        # creating label vector\n",
    "        base.drop(columns=['Trial', 'MS Score'], inplace=True)\n",
    "        data.drop(columns=['Trial', 'MS Score'], inplace=True)\n",
    "        misc_data.drop(columns=['Trial'], inplace=True) # remove unnecessary data columns\n",
    "\n",
    "        subject_code = np.zeros(35)\n",
    "        subject_code[int(crop_name(t)[:2]) - 1] = 1\n",
    "        extended_code = np.tile(subject_code, (data.shape[0], 1))\n",
    "        # create one hot encoding for subject ID\n",
    "\n",
    "        offset_data = []    # to store each offset before concatenating them all to the current window\n",
    "        for i in range(n):  # loop through n previous windows\n",
    "            length = min(len(data), i + 1)  # account for short trials with fewer windows than n\n",
    "            offset = np.repeat(base.values, length, axis=0)     # baseline values for all the windows that don't have enough previous data \n",
    "            cropped_data = data.values[:(-1 * (i + 1))]     # select previous data in lower diagonal\n",
    "            offset = np.concatenate((offset, cropped_data), axis=0)\n",
    "            offset_data.append(offset)\n",
    "        # this loop iterates through the n=i previous data for an entire trial \n",
    "        # is does NOT loop through each window adding their respective offsets\n",
    "        offset_data.append(data.values)\n",
    "        offset_data.append(misc_data.values)\n",
    "        offset_data.append(extended_code)\n",
    "        # concatenate previous vectors, current vector, and misc vector\n",
    "        X.append(np.concatenate(offset_data, axis=1))\n",
    "    return np.array(X), np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9p/n09d6qfj5d99hs5gvj02l5kr0000gn/T/ipykernel_19844/632829838.py:98: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array(X), np.array(Y)\n"
     ]
    }
   ],
   "source": [
    "misc = ['Susceptibility', 'Gender', 'Age', 'APS', 'Task', 'Sleep_hours', 'Normal_sleep', 'Other_naps', 'Stress_level', 'Fatigue_level', 'Amount_of_exercies', 'Eat_in_12', 'Last_eat_time']\n",
    "accel = [a[:2] for a in accelerations]\n",
    "features = ['MNF', 'MDF', 'PKF', 'MNP', 'TTP', 'FR', 'PSR', 'FMN', 'FMD', 'AAV', 'MAV', 'MMAV', 'SSI', 'VAR', 'ZC', 'SSC', 'WL', 'AAC', 'WAMP']\n",
    "# select which misc data, acceleration type, and features to extract\n",
    "\n",
    "X, Y = extract_time_offset_features(features=features, accel=accel, misc=misc, n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_table = pd.read_csv('feature_tables/features.csv')\n",
    "\n",
    "def custom_kfold(k):\n",
    "    trial_names = feat_table['Trial'].unique()\n",
    "    abs_index = {}\n",
    "    group_count = defaultdict(int)\n",
    "    for count, t in enumerate(trial_names):\n",
    "        abbr = t[:10]\n",
    "        if group_count[abbr] == 0:\n",
    "            abs_index[abbr] = count\n",
    "        group_count[abbr] += 1\n",
    "    \n",
    "    group_names = list(group_count.keys())\n",
    "    random.shuffle(group_names)\n",
    "\n",
    "    fold = 0\n",
    "    fold_mat = []\n",
    "    splits = []\n",
    "    for i in range(k):\n",
    "        fold_mat.append(list())\n",
    "\n",
    "    for n in group_names:\n",
    "        for j in range(group_count[n]):\n",
    "            fold_mat[fold].append(abs_index[n] + j)\n",
    "            fold += 1\n",
    "            fold %= k\n",
    "\n",
    "    for i in range(k):\n",
    "        test = fold_mat[i]\n",
    "        train = []\n",
    "        for j in range(k):\n",
    "            if j != i:\n",
    "                train += fold_mat[j]\n",
    "        splits.append((train, test))\n",
    "\n",
    "    return splits\n",
    "\n",
    "def cross_validation(X, Y, k=5, reps=50, trees=100, loss='gini', calc_auc=False):\n",
    "    kf = custom_kfold(k)\n",
    "    super_sum = 0\n",
    "    f_ones = 0\n",
    "    auc_scores = 0\n",
    "    repetitions = reps\n",
    "    for i in range(repetitions): # simulate many times to find the true average\n",
    "        running_confusion = []\n",
    "        for train, test in kf:\n",
    "            auc = 0\n",
    "            try:\n",
    "                X_train = np.concatenate(X[train], axis=0)\n",
    "                X_train = np.vstack(X_train)\n",
    "                X_test = np.concatenate(X[test], axis=0)\n",
    "                X_test = np.vstack(X_test)\n",
    "                Y_train = np.concatenate(Y[train], axis=0)\n",
    "                Y_train = np.vstack(Y_train)\n",
    "                Y_test = np.concatenate(Y[test], axis=0)\n",
    "                Y_test = np.vstack(Y_test)\n",
    "            except:\n",
    "                print('Error. Continuing.')\n",
    "                continue\n",
    "            # index data by train/test split and concatenate the samples of all the trials\n",
    "            pipe = Pipeline([('rf', RandomForestClassifier(criterion=loss, n_estimators=trees, random_state=42))])    # define model\n",
    "            pipe.fit(X_train, Y_train.ravel())  # train model\n",
    "            pred = pipe.predict(X_test) # predict on test set\n",
    "            if calc_auc:\n",
    "                probas = pipe.predict_proba(X_test) \n",
    "            score = pipe.score(X_test, Y_test)\n",
    "            f1 = f1_score(pred, Y_test, average='macro')\n",
    "            cm = confusion_matrix(Y_test, pred)\n",
    "            if calc_auc:\n",
    "                auc = roc_auc_score(Y_test.ravel(), probas[:, 1], multi_class='ovr', average='macro')\n",
    "            # if len(cm) == 2:  # to account for ternary confusion matrices that have 0 predictions in a class\n",
    "            #     cm = np.pad(cm, ((0,1), (0,1)), mode='constant')  \n",
    "            # running_confusion.append(cm)\n",
    "            super_sum += score\n",
    "            f_ones += f1\n",
    "            auc_scores += auc\n",
    "        # confusion = sum(running_confusion)\n",
    "        # if i == 0:  # print confusion matrix for the first simulation\n",
    "        #     plt.imshow(confusion, cmap='Blues')\n",
    "        #     for i in range(confusion.shape[0]):\n",
    "        #         for j in range(confusion.shape[1]):\n",
    "        #             plt.text(j, i, confusion[i, j], ha='center', va='center', color='black')\n",
    "        #     plt.xticks(np.arange(confusion.shape[1]))\n",
    "        #     plt.yticks(np.arange(confusion.shape[0]))\n",
    "\n",
    "    accuracy = super_sum / (k * repetitions)\n",
    "    f_one_score =  f_ones / (k * repetitions)\n",
    "    area_under_curve = auc_scores / (k * repetitions)\n",
    "    return accuracy, f_one_score, area_under_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = cross_validation(X, Y, 5, 200, trees=50, loss='entropy', calc_auc=True)\n",
    "print([round(x, 4) for x in results])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pd.read_csv('Feature_Tables/features.csv')\n",
    "misc = ['Susceptibility', 'Gender', 'Age', 'APS', 'Task', 'Sleep_hours', 'Normal_sleep', 'Other_naps', 'Stress_level', 'Fatigue_level', 'Amount_of_exercies', 'Eat_in_12', 'Last_eat_time']\n",
    "table.drop(columns=['Time', 'Trial', 'MS Score'], inplace=True)\n",
    "table.drop(columns=misc, inplace=True)\n",
    "feat = table.columns\n",
    "print(len(feat))\n",
    "feature_names = ['1 - ' + x for x in feat]\n",
    "# create list of feature names in string form\n",
    "\n",
    "feature_names.extend(['2 - ' + x for x in feat])\n",
    "feature_names.extend(['3 - ' + x for x in feat])\n",
    "feature_names.extend(['4 - ' + x for x in feat])\n",
    "feature_names.extend(['5 - ' + x for x in feat])\n",
    "feature_names.extend(['6 - ' + x for x in feat])\n",
    "feature_names.extend(['7 - ' + x for x in feat])\n",
    "feature_names.extend(['8 - ' + x for x in feat])\n",
    "feature_names.extend(['9 - ' + x for x in feat])\n",
    "feature_names.extend(['10 - ' + x for x in feat])\n",
    "feature_names.extend(['0 - ' + x for x in feat])\n",
    "feature_names.extend(misc)\n",
    "feature_names.extend(['Subject Code' for i in range(35)])\n",
    "print(feature_names[55])\n",
    "# crude, but creates feature names for offset windows\n",
    "\n",
    "X_train = np.concatenate(X, axis=0)\n",
    "X_train = np.vstack(X_train)\n",
    "y_train = np.concatenate(Y, axis=0)\n",
    "y_train = np.vstack(y_train)\n",
    "# train with all the data\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators=400, random_state=0)   # define model\n",
    "forest.fit(X_train, y_train.ravel())    # train model\n",
    "\n",
    "start_time = time.time()    # timer\n",
    "importances = forest.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in forest.estimators_], axis=0)\n",
    "elapsed_time = time.time() - start_time\n",
    "print(len(importances))\n",
    "print(len(feature_names))\n",
    "feature_importances = pd.Series(importances, index=feature_names)\n",
    "# calculate feature importances\n",
    "\n",
    "print(f\"Elapsed time to compute the importances: {elapsed_time:.3f} seconds\")\n",
    "for f, i in zip(feature_names, feature_importances):\n",
    "    print(f)\n",
    "for f, i in zip(feature_names, feature_importances):\n",
    "    print(100 * i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "misc = ['Susceptibility', 'Gender', 'Age', 'APS', 'Task', 'Sleep_hours', 'Normal_sleep', 'Other_naps', 'Stress_level', 'Fatigue_level', 'Amount_of_exercies', 'Eat_in_12', 'Last_eat_time']\n",
    "\n",
    "misc.extend(features)\n",
    "misc.append('Random')\n",
    "for i in range(11):\n",
    "    indices = [index for index, string in enumerate(feature_names) if string[:2] == str(i) + ' ']\n",
    "    total = sum(feature_importances[i] for i in indices)\n",
    "    print(i)\n",
    "\n",
    "for i in range(11):\n",
    "    indices = [index for index, string in enumerate(feature_names) if string[:2] == str(i) + ' ']\n",
    "    total = sum(feature_importances[i] for i in indices)\n",
    "    print(round(100 * total, 5))\n",
    "# calculate importance sums for acceleration types and individual features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [2:08:32<00:00, 1285.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters\n",
      "Num Estimators: 50.0\n",
      "Loss Function: entropy\n",
      "Score: 0.7831160305073366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def hp_grid_search(X, Y, tree=[50, 300]):\n",
    "    criterion = ['gini', 'entropy']   # create list on 10^x scale\n",
    "    tree_range = np.linspace(tree[0], tree[1], 6)   # create list on 10^x scale\n",
    "    best_score = 0\n",
    "    best_params = [0, 0]\n",
    "\n",
    "    for t in tqdm(tree_range):\n",
    "        for c in criterion:\n",
    "            score = cross_validation(X, Y, 5, 100, int(t), c)[0]    # accuracy score\n",
    "            if score > best_score:\n",
    "                # update best\n",
    "                best_score = score\n",
    "                best_params[0] = t\n",
    "                best_params[1] = c\n",
    "    return best_params, best_score\n",
    "    # [C, gamma]\n",
    "\n",
    "params, accuracy = hp_grid_search(X, Y)\n",
    "print(\"Best parameters\")\n",
    "print(\"Num Estimators:\", params[0])\n",
    "print(\"Loss Function:\", params[1])\n",
    "print('Score:', accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
